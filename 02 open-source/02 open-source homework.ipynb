{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cc92fa-9db2-42a1-889e-1aa7fd3d62dc",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7190a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313d39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab95af4-ea55-4504-8686-9ed3d0f52641",
   "metadata": {},
   "source": [
    "### Preparing OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec3b552-ad1e-4325-935e-99ec7c162327",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key='ollama',\n",
    "    base_url='http://localhost:11434/v1/',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0147dec0-5df5-470b-b7e5-464e52ef3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt: str, temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response from the 'gemma:2b' model based on the provided prompt.\n",
    "\n",
    "    This method sends a user message to the 'gemma:2b' model via a chat completion request,\n",
    "    specifying the desired level of randomness through the `temperature` parameter,\n",
    "    and returns the content of the first choice made by the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt : str\n",
    "        The user's message or prompt to which the model will respond.\n",
    "    temperature : float, optional\n",
    "        Controls the randomness of the model's output, where lower values make the output more deterministic.\n",
    "        Default is 1.0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The content of the model's response as a string.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model='gemma:2b',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d6ed7-d8e6-4b8e-bc01-53095928ced1",
   "metadata": {},
   "source": [
    "### Preparing Huggingface environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c9326a-0016-421d-bf95-fc9d844d1e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/nikolai/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('../.env')\n",
    "login(token=os.getenv('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f4bfd-999d-4fe2-b7fc-cb568ceaa060",
   "metadata": {},
   "source": [
    "## [Homework questions](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da35eed-cf16-41c3-8f73-5351ad9808ca",
   "metadata": {},
   "source": [
    "### [Q1. Running Ollama with Docker](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md#q1-running-ollama-with-docker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f96a020-394a-4a86-be63-936aab194477",
   "metadata": {},
   "source": [
    "Ollama version: 0.1.48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741b42f-ef5e-4cbc-ae0d-fb94e65d2b35",
   "metadata": {},
   "source": [
    "### [Q2. Downloading an LLM](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md#q2-downloading-an-llm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4f41e19-eb29-4d1f-8f6b-b6767f2ce729",
   "metadata": {},
   "source": [
    "{\"schemaVersion\":2,\"mediaType\":\"application/vnd.docker.distribution.manifest.v2+json\",\"config\":{\"mediaType\":\"application/vnd.docker.container.image.v1+json\",\"digest\":\"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\"size\":483},\"layers\":[{\"mediaType\":\"application/vnd.ollama.image.model\",\"digest\":\"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\"size\":1678447520},{\"mediaType\":\"application/vnd.ollama.image.license\",\"digest\":\"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\"size\":8433},{\"mediaType\":\"application/vnd.ollama.image.template\",\"digest\":\"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\"size\":136},{\"mediaType\":\"application/vnd.ollama.image.params\",\"digest\":\"sha256:22a838ceb7fb22755a3b0ae9b4eadde629d19be1f651f73efb8c6b4e2cd0eea0\",\"size\":84}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55f6d19-1edf-4175-903b-2fcbb6117fb5",
   "metadata": {},
   "source": [
    "### [Q3. Running the LLM](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md#q3-running-the-llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aebc2d56-a335-4c48-87cb-1992f3c88185",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '10*10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "271bf7c3-3967-43b3-a41a-eaa5eb042bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure. Here's a rewritten response that adheres to the given guidelines:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13137402-acb5-41c9-b2a5-2fc7a16f26af",
   "metadata": {},
   "source": [
    "### [Q4. Donwloading the weights](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md#q4-donwloading-the-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59da4c-d5b3-4af1-affe-46bbafdf205d",
   "metadata": {},
   "source": [
    "Q4 answer: 1.6 G (best closest is 1.7 G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19403b1e-b382-446e-9eca-1253b965a606",
   "metadata": {},
   "source": [
    "### [Q5. Adding the weights](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md#q5-adding-the-weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3460e6a-0b45-4bf0-a780-f948736007bc",
   "metadata": {},
   "source": [
    "Q5 answer: ollama_files /root/.ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379de94-807b-4368-9ed5-674943b4ec03",
   "metadata": {},
   "source": [
    "### [Q6. Serving it](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/cohorts/2024/02-open-source/homework.md#q6-serving-it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79cb800d-cb5d-4574-b863-a7496c91d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'What\\'s the formula for energy?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8bef63-4dab-43ee-b749-f9f4b981a03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6 answer: 281 completion tokens recieved.\n"
     ]
    }
   ],
   "source": [
    "result = llm(prompt, temperature=0.0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "tokens = tokenizer.encode(result)\n",
    "\n",
    "print(f'Q6 answer: {len(tokens)} completion tokens recieved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
